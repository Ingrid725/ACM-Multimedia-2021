<!DOCTYPE html>
<html lang="en" class="js csstransforms3d">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Hugo 0.88.1" />
    <meta name="robots" content="noindex, nofollow, noarchive, noimageindex">
    <meta name="description" content="Documentation for Hugo Relearn Theme">
    <meta name="author" content="Sören Weber">
    
    <title>ACM Multimedia 2021</title>
    <link href="./css/nucleus.css?1637301373" rel="stylesheet">
    <link href="./css/fontawesome-all.min.css?1637301373" rel="stylesheet">
    <link href="./css/featherlight.min.css?1637301373" rel="stylesheet">
    <link href="./css/perfect-scrollbar.min.css?1637301373" rel="stylesheet">
    <link href="./css/auto-complete.css?1637301373" rel="stylesheet">
    <link href="./css/theme.css?1637301373" rel="stylesheet">
    <link href="./css/theme-relearn.css?1637301373" rel="stylesheet">
    <link href="./css/variant.css?1637301373" rel="stylesheet">
    <link href="./css/print.css?1637301373" rel="stylesheet" media="print">
    <script src="./js/jquery.min.js?1637301373"></script>
    <style>
      :root #header + #content > #left > #rlblock_left{
        display:none !important;
      }
    </style>
  </head>

  
  <body class="" data-url="./">
    <script>
      var index_url="./index.json";
      var root_url="./";
      var baseUri=root_url.replace(/\/$/, '');
    </script>
    <nav id="sidebar" class="showVisitedLinks">
      <div id="header-wrapper">
        <div id="header">
          <a id="logo"
            href="./"
            style="
              color: #fdfcfc;
              font-family: 'Work Sans', 'Helvetica', 'Tahoma', 'Geneva', 'Arial', sans-serif;
              font-size: 30px;
              font-weight: 300;
              margin-top: -2px;
              text-transform: uppercase;
            ">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 64.044 64.044"
              style="
                color: #fdfcfc;
                height: 65px;
                margin-bottom: -20px;
                margin-top: -20px;
                width: 65px;
              ">
              <path d="M46.103 136.34c-.642-.394-1.222-2.242-1.98-2.358-.76-.117-1.353.506-1.618 1.519-.266 1.012-.446 4.188.173 5.538.213.435.482.787 1.03.845.547.057.967-.504 1.45-1.027.482-.523.437-.9 1.142-.612.705.289 1.051.4 1.586 1.229.535.828 1.085 4.043.868 5.598-.241 1.458-.531 2.8-.59 4.088.26.075.517.148.772.217 2.68.724 5.373 1.037 7.873.02.001-.028.01-.105.008-.11-.048-.165-.18-.41-.36-.698-.18-.29-.414-.645-.586-1.114a3.212 3.212 0 0 1-.125-1.735c.056-.21.153-.342.249-.475 1.237-1.193 2.932-1.373 4.244-1.384.557-.004 1.389.016 2.198.255.809.239 1.706.724 2.068 1.843.187.578.114 1.17-.043 1.623-.153.438-.369.783-.545 1.091-.178.31-.329.6-.401.821-.007.02-.003.071-.005.094 2.256 1.008 4.716.91 7.189.398.55-.114 1.11-.247 1.673-.377.344-1.085.678-2.145.852-3.208.124-.752.158-2.311-.078-3.538-.118-.613-.306-1.15-.52-1.489-.221-.349-.413-.501-.747-.538-.243-.027-.51.013-.796.098-.67.223-1.33.606-1.966.76l-.008.002-.109.032c-.556.152-1.233.158-1.797-.36-.556-.51-.89-1.367-1.117-2.596-.283-1.528-.075-3.279.89-4.518l.071-.09h.07c.65-.71 1.485-.802 2.16-.599.706.213 1.333.629 1.772.84.736.354 1.185.319 1.475.171.291-.148.5-.439.668-.955.332-1.017.301-2.819.022-4.106-.148-.684-.13-1.292-.13-1.883-1.558-.463-3.067-.982-4.574-1.208-1.128-.169-2.263-.173-3.298.164-.13.046-.256.095-.38.15-.373.164-.633.342-.805.52-.077.098-.081.105-.087.21-.004.068.031.289.13.571.1.282.256.634.467 1.03.279.524.448 1.063.431 1.618a2.12 2.12 0 0 1-.499 1.309 1.757 1.757 0 0 1-.62.51h-.002c-.515.291-1.107.404-1.723.464-.86.083-1.787.026-2.598-.097-.806-.123-1.47-.28-1.948-.555-.444-.256-.79-.547-1.037-.925a2.273 2.273 0 0 1-.356-1.301c.029-.837.403-1.437.625-1.897.111-.23.191-.433.236-.583.045-.15.044-.25.046-.24-.005-.029-.127-.355-1.015-.741-1.138-.495-2.322-.673-3.533-.668h-.015a9.711 9.711 0 0 0-.521.016h-.002c-1.163.057-2.35.308-3.541.569.383 1.531.79 2.753.818 4.502-.096 1.297.158 2.114-1.03 2.935-.85.588-1.508.729-2.15.335" style="fill:#3d414d;fill-opacity:1;stroke:none;stroke-width:1.03763;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" transform="translate(-40.698 -95.175)"/>
              <path d="M61.472 101.34v.002c-.3-.003-.603.01-.894.04-.544.055-1.39.165-1.778.306-1.238.364.13 2.344.41 2.913.28.569.285 2.03.14 2.134-.144.103-.375.261-.934.345-.56.084.03-.037-1.589.086-1.62.122-5.506.29-8.265.248-.022.26-.036.521-.097.808-.309 1.442-.63 3.163-.494 4.074.071.473.168.65.414.8.23.14.737.235 1.62-.004.834-.227 1.3-.442 1.887-.456.595-.016 1.555.472 1.965.717.411.245-.03-.008.002 0s.128.05.176.102c.049.053-.276-.523.104.199.379.721.72 3.256.002 4.68-.46.913-1.01 1.49-1.64 1.711-.63.22-1.229.067-1.734-.135-.881-.353-1.584-.7-2.205-.647-1.199 1.94-1.186 4.17-.6 6.602.097.397.212.814.327 1.23 2.68-.556 5.542-1.016 8.337.132 1.064.437 1.73 1.015 1.902 1.857.169.831-.193 1.508-.438 1.986-.122.238-.23.46-.307.642-.07.164-.096.28-.104.324.069.429.309.723.686.945.385.227.89.355 1.35.423.723.104 1.567.152 2.287.086.693-.064 1.032-.338 1.241-.544a2.447 2.447 0 0 0 .303-.437.175.175 0 0 0 .013-.035c-.004-.066-.037-.246-.195-.527-.46-.816-.87-1.595-.817-2.51.028-.476.218-.938.529-1.288.304-.343.698-.586 1.186-.79 1.442-.606 2.96-.609 4.372-.409 1.525.216 2.963.679 4.378 1.083.226-2.09.784-3.9.592-5.77-.058-.565-.287-1.333-.598-1.827-.32-.508-.59-.717-1.036-.642-.648.11-1.472.935-2.707 1.078-.791.092-1.494-.267-1.95-.86-.45-.583-.678-1.335-.78-2.101-.202-1.525.031-3.229.89-4.27.615-.747 1.45-.887 2.15-.74.687.145 1.307.492 1.857.745v-.002c.546.252 1.033.388 1.281.344a.547.547 0 0 0 .353-.188c.113-.124.242-.35.384-.75.604-1.712.206-3.68-.303-5.654-.667.145-1.336.293-2.018.413-1.341.236-2.73.392-4.136.273-.656-.055-1.695-.085-2.58-.476-.442-.195-.903-.514-1.157-1.093-.259-.591-.205-1.313.08-2.014.223-.64 1.082-2.178.692-2.585-.391-.407-1.651-.56-2.554-.571z" style="fill:#3d414d;fill-opacity:1;stroke:none;stroke-width:.992837;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" transform="translate(-40.698 -95.175)"/>
              <path d="M83.128 98.116c-.484 1.875-1.057 3.757-2.486 5.033-.638.57-1.13.666-1.483.548-.401-.134-.715-.506-1.058-.973-.338-.461-.655-.97-1.076-1.332-.192-.165-.404-.315-.683-.38-.279-.066-.599-.02-.9.102-.489.196-.89.58-1.28 1.047a6.1 6.1 0 0 0-.985 1.632c-.234.591-.356 1.174-.277 1.713.072.487.392.977.905 1.185.463.187.926.156 1.36.154.433 0 .843.01 1.242.147.55.189.79.736.822 1.368.034.66-.145 1.412-.393 1.988l-.008.021c-.74 1.705-1.946 2.893-3.004 4.349l-.664.915.979.099c.924.092 1.788.26 2.468.675.46.281 1.806 1.205 2.794 2.222.497.513.888 1.031 1.047 1.502.078.231.095.422.05.6a.93.93 0 0 1-.345.474c-.301.223-.606.395-.864.532l-.354.186c-.107.058-.189.087-.345.228a.637.637 0 0 1 .062-.045l-.064.041-.209.236-.103.343s.003.126.007.152c.003.017.003.007.004.015v.002c.016.195.061.307.133.476a4.1 4.1 0 0 0 .32.596 5.7 5.7 0 0 0 2.8 2.258c.284.108.908.321 1.548.36.33.02.59.015.912-.13h.002c.08-.037.228-.095.382-.281.153-.186.19-.355.212-.445l.019-.075.003-.078c.023-.585-.037-1.296.072-1.899.153-.657.435-.956 1.009-.909 2.771.239 4.74 1.955 6.693 3.83l.742.714.279-1.155c.55-2.29 1.093-4.464 2.928-5.977.692-.57 1.184-.642 1.527-.509.39.151.676.536.996.995.319.458.605.926 1.07 1.212.194.119.464.232.784.209.32-.024.638-.163.988-.384 1.022-.645 1.778-1.756 2.086-2.942.136-.522.102-.991-.046-1.301-.158-.334-.433-.553-.754-.707-.653-.314-1.468-.373-2.094-.486-.825-.15-1.22-.475-1.345-.878-.13-.417 0-.953.335-1.61.6-1.173 1.887-2.602 3.13-3.911l.498-.526-.449-.432c-1.545-1.49-3.163-3.01-5.252-3.715h-.002c-.473-.16-1.097-.413-1.73-.424h-.003c-.311-.004-.596.04-.883.24v.002c-.22.155-.483.537-.583.937l-.008.036-.006.038c-.116.773-.06 1.467-.217 1.995-.063.212-.198.418-.359.507-.202.111-.492.153-.976.072-.582-.097-1.978-.69-3.021-1.503-.523-.407-.934-.85-1.117-1.3a1.153 1.153 0 0 1-.083-.63c.03-.184.1-.477.308-.593.21-.116.941-.32 1.377-.642h.002c.192-.141.403-.367.518-.64.114-.275.127-.526.123-.774-.006-.142-.036-.192-.08-.3a8.417 8.417 0 0 0-3-3.027c-1.226-.725-2.585-1.135-3.927-1.539-.434-.12-.844-.111-1.02.466zm.912.947c1.186.364 2.357.718 3.345 1.303 1.035.612 1.864 1.488 2.507 2.528-.514.263-1.095.5-1.44.79-.345.29-.729.914-.815 1.434-.084.509 0 .968.155 1.347.301.74.85 1.276 1.44 1.735 1.18.92 2.554 1.545 3.47 1.698.604.1 1.186.088 1.739-.216.594-.327.935-.911 1.088-1.427.264-.884.193-1.664.262-2.17h.1c.3.006.926.206 1.417.371 1.646.554 3.044 1.773 4.431 3.089-1.102 1.174-2.222 2.423-2.888 3.73-.42.823-.73 1.789-.453 2.687.283.913 1.1 1.415 2.138 1.603.691.126 1.472.226 1.84.403.19.091.258.182.278.223.03.064.058.075-.023.387-.21.804-.761 1.598-1.413 2.01-.247.155-.365.183-.407.187-.042.003-.061.002-.172-.066-.144-.088-.455-.473-.772-.929-.317-.454-.714-1.07-1.452-1.356-.783-.304-1.776-.022-2.713.75-1.942 1.6-2.626 3.764-3.146 5.8-1.802-1.676-3.772-3.138-6.589-3.517h-.002c-.346-.095-1.013-.031-1.293.143-.735.501-1.005 1.132-1.168 2.007-.125.69-.082 1.216-.074 1.659-.055.006-.046.01-.104.006-.42-.026-1.035-.215-1.244-.295-.947-.361-1.774-1.006-2.314-1.857-.054-.085-.072-.132-.109-.2l.027-.016c.284-.15.656-.36 1.045-.648.44-.327.789-.798.93-1.35a2.4 2.4 0 0 0-.068-1.379c-.254-.751-.753-1.353-1.295-1.911-1.09-1.124-2.452-2.049-2.99-2.378-.609-.372-1.303-.44-1.981-.56.875-1.094 1.878-2.251 2.596-3.921.294-.823.543-1.907.513-2.658-.049-.97-.489-2.013-1.52-2.367-.579-.2-1.131-.204-1.58-.203-.45.002-.786-.006-.97-.08h-.002c-.264-.107-.236-.108-.268-.33-.025-.17.021-.553.183-.962a4.67 4.67 0 0 1 .725-1.192c.29-.348.617-.59.705-.626.142-.057.176-.05.22-.04.045.011.127.052.263.17.235.201.56.671.92 1.161.354.484.791 1.08 1.543 1.33.8.267 1.784-.052 2.671-.846 1.594-1.424 2.235-3.317 2.714-5.051zm11.705 7.023c-.02.014.042-.002.042 0l-.008.035c.05-.2-.028-.04-.034-.035zM79.472 122.45a.198.198 0 0 1 .005.023v.014c-.002-.01-.003-.03-.005-.037zm-.29.732-.006.01-.044.027c.016-.01.033-.024.05-.036z" style="color:#000;fill:#3d414d;stroke-width:1.02352;-inkscape-stroke:none" transform="translate(-40.698 -95.175)"/>
              <path d="M76.694 128.845c-.85-.012-1.668.253-2.434.67-.01.592-.015 1.17.109 1.772.323 1.573.422 3.553-.07 5.147-.247.804-.684 1.535-1.347 1.891-.663.356-1.467.296-2.362-.159-.522-.266-1.059-.62-1.487-.757-.223-.072-.392-.096-.522-.069-.13.027-.232.094-.362.27-.53.719-.681 1.823-.497 2.876.177 1.012.418 1.438.543 1.56.143.137.26.154.604.055.548-.158 1.523-.857 2.573-.972l.02-.002.5.058c.686.081 1.247.562 1.622 1.19.372.62.591 1.37.73 2.136.279 1.532.25 3.16.083 4.232-.14.91-.394 1.72-.632 2.53 1.719-.385 3.485-.692 5.307-.36 1.174.214 2.749.574 3.762 1.977l.088.122.046.159c.162.551.16 1.114.024 1.578-.13.45-.348.772-.533 1.023-.181.246-.336.444-.437.606-.102.16-.141.275-.145.336-.01.17 0 .197.07.315.057.1.186.242.39.366.408.246 1.106.414 1.843.45a7.842 7.842 0 0 0 2.174-.21 4.28 4.28 0 0 0 .822-.296c.218-.106.385-.242.377-.233l.029-.031c.025-.035.05-.072.05-.068 0-.004 0-.017-.003-.05a2.733 2.733 0 0 0-.21-.579c-.26-.548-.839-1.333-.822-2.46.01-.657.27-1.21.598-1.576.32-.357.696-.575 1.074-.736.759-.323 1.57-.418 2.054-.458 1.653-.136 3.252.296 4.755.765.457.142.905.29 1.352.434.325-2.258.902-4.247.598-6.217-.071-.46-.25-1.169-.486-1.684-.238-.518-.495-.762-.675-.779-.351-.032-.716.14-1.174.418-.457.277-1.005.665-1.695.742-.745.082-1.406-.291-1.84-.908-.428-.608-.653-1.394-.754-2.196-.203-1.596.016-3.377.794-4.493.568-.813 1.358-.984 2.024-.835.65.146 1.243.51 1.769.779.524.267.99.413 1.237.365a.527.527 0 0 0 .346-.2c.11-.132.235-.373.37-.798.612-1.918.27-3.894-.246-6.054-2.815-.851-5.49-1.534-8.089-.267a.727.727 0 0 0-.223.148c-.024.028-.018.021-.026.056.001-.003-.01.178.07.44.162.522.611 1.29.911 1.978l.004.009.029.063.024.084V133c.162.635.016 1.297-.274 1.727-.272.404-.618.636-.952.81-.675.353-1.399.484-1.724.533a5.888 5.888 0 0 1-3.973-.795c-.512-.311-.876-.594-1.133-1.02-.282-.466-.318-1.084-.172-1.557.252-.814.715-1.266.971-1.89a.663.663 0 0 0 .047-.14c.001-.013 0-.006-.007-.037a.761.761 0 0 0-.184-.268c-.264-.267-.865-.595-1.54-.826-1.356-.462-3.07-.659-3.583-.686-.062-.002-.121-.006-.178-.006z" style="fill:#3d414d;fill-opacity:1;stroke:none;stroke-width:.991342;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" transform="translate(-40.698 -95.175)"/>
            </svg>
            ACM Multimedia
          </a>
        </div>
        <div class="searchbox">
          <label for="search-by"><i class="fas fa-search"></i></label>
          <input data-search-input id="search-by" type="search" placeholder="Search...">
          <span data-search-clear=""><i class="fas fa-times"></i></span>
        </div>
        <script src="./js/lunr.min.js?1637301373"></script>
        <script src="./js/auto-complete.js?1637301373"></script>
        <!-- hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic: -->
        <!-- https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72 -->
        <script src="./js/search.js?1637301373"></script>
      </div>
      <div class="highlightable">
        <div id="shortcuts">
          <div class="nav-title">list</div>
          <ul>
            <li><a class="padding" href="https://ingrid725.github.io/ACM-Multimedia-2021/">homepage</a></li>
            <li><a class="padding" href="https://pan.baidu.com/s/13afGYKVIAxDLKkBtq-aqTw">courseware download(Baiduyun, extraction code:gxu7)</a></li>
            <li><a class="padding" href="https://drive.google.com/drive/folders/1-x4kBqYyiqL6DVZ9YnHpN-WhH1HYCcXb?usp=sharing">courseware download(Google Cloud)</a></li>
          </ul>
        </div>
      </div>
    </nav>
    <div id="body">
      <div id="overlay"></div>
      <div class="padding highlightable">
        <div id="head-tags">
        </div>
        <main id="body-inner">
<span id="sidebar-toggle-span">
<a href="#" id="sidebar-toggle" data-sidebar-toggle=""><i class="fas fa-bars"></i> navigation</a>
</span>


<h1 id="tutorial-proposal">Tutorial proposal</h1>
<h1 id="for-acm-multimedia-2021">for ACM Multimedia 2021</h1>
<h4 id="jie-chen-qixiang-ye-xiaoshan-yang-s-kevin-zhou-xiaopeng-hong-li-zhang">Jie Chen, Qixiang Ye, Xiaoshan Yang, S. Kevin Zhou, Xiaopeng Hong, Li Zhang</h4>
<p> </p>
<blockquote>
<h3 id="tutorial-title-few-shot-learning-for-multi-modality-tasks">Tutorial title: Few-shot Learning for Multi-Modality Tasks</h3>
</blockquote>
<ul>
<li>Few-shot Learning: Fundamental (Li Zhang)<a href="">(ppt)</a></li>
<li>Few-shot Learning for Medical Image Processing (S. Kevin Zhou)<a href="https://docs.qq.com/pdf/DZmxtT0hpd0VhRkNu">(ppt)</a></li>
<li>Few-shot Semantic Segmentation (Qixiang Ye)<a href="">(ppt)</a></li>
<li>Few-shot Incremental Learning (Xiaopeng Hong)<a href="https://docs.qq.com/pdf/DZmtYRlVrT2lyWkdS">(ppt)</a></li>
<li>Few-shot Learning for Multi-modality Tasks (Xiaoshan Yang)<a href="https://docs.qq.com/pdf/DZlVBSld2eEdBcEla">(ppt)</a></li>
<li>Few-shot Medical Image Segmentation(Jie Chen)<a href="https://docs.qq.com/pdf/DZlp3RlB6T01xb1hL">(ppt)</a></li>
</ul>
<blockquote>
<h3 id="proposers">Proposers</h3>
</blockquote>
<ul>
<li>Li Zhang, Associate Professor, Fudan University, China, <a href="mailto:lizhangfd@fudan.edu.cn">lizhangfd@fudan.edu.cn</a></li>
<li>S. Kevin Zhou, Distinguished Professor, University of Science and Technology of China, <a href="mailto:s.kevin.zhou@gmail.com">s.kevin.zhou@gmail.com</a></li>
<li>Qixiang Ye, Tenured Professor, University of Chinese Academy of Sciences, <a href="mailto:qxye@ucas.ac.cn">qxye@ucas.ac.cn</a></li>
<li>Xiappeng Hong, Professor, Xi’an Jiaotong University, China, <a href="mailto:hongxiaopeng@ieee.org">hongxiaopeng@ieee.org</a></li>
<li>Xiaoshan Yang, Associate Professor, University of Chinese Academy of Sciences,  <a href="mailto:xiaoshan.yang@nlpr.ia.ac.cn">xiaoshan.yang@nlpr.ia.ac.cn</a></li>
<li>Jie Chen, Associate Professor, Peking University, China, <a href="mailto:jiechen2019@pku.edu.cn">jiechen2019@pku.edu.cn</a></li>
</ul>
<blockquote>
<h3 id="short-bio">Short bio</h3>
</blockquote>
<p><img src="./Li_Zhang.jpg" alt="Li Zhang"></p>
<ul>
<li>Li Zhang is a tenure-track Associate Professor at the School of Data Science, Fudan University. He was elected to the Shanghai Science &amp; Technology 35 Under 35. Previously, he was a Research Scientist at Samsung AI Center Cambridge, and a Postdoctoral Research Fellow at the University of Oxford. Prior to joining Oxford, he read his PhD in computer science at Queen Mary University of London. The aim of his research group at Fudan is to make the machine see and empower the next generation AI by striving to achieve the most universal representation of understanding objects, scene and motion with mathematical models of neural networks.</li>
</ul>
<p><img src="./Kevin_Zhou.jpg" alt="Kevin Zhou"></p>
<ul>
<li>S. Kevin Zhou Professor S. Kevin Zhou obtained his PhD degree from University of Maryland, College Park. He is a Distinguished Professor and founding Executive Dean of School of Biomedical Engineering at University of Science and Technology of China and an adjunct professor at Chinese Academy of Sciences and Chinese University of Hong Kong (Shenzhen). He was a Principal Expert and a Senior R&amp;D director at Siemens Healthcare. Dr. Zhou has published 200+ book chapters and peer-reviewed journal and conference papers, registered 140+ granted patents, written two research monographs, and edited three books. His two most recent books are entitled “Medical Image Recognition, Segmentation and Parsing: Machine Learning and Multiple Object Approaches, SK Zhou (Ed.)” and “Deep Learning for Medical Image Analysis, SK Zhou, H Greenspan, DG Shen (Eds.).” He has won multiple awards including R&amp;D 100 Award (Oscar of Invention), Siemens Inventor of the Year, and UMD ECE Distinguished Aluminum Award. He has been an associate editor for IEEE Transactions on Medical Imaging and Medical Image Analysis, a program co-chair for MICCAI2020, an area chair for AAAI, ICCV, CVPR, MICCAI, and NeurIPS, an executive treasurer and board member of the MICCAI Society. He is a Fellow of National Academy of Inventors (NAI), IEEE, and AIMBE.</li>
</ul>
<p><img src="./Qixiang_Ye.jpg" alt="Qixiang Ye"></p>
<ul>
<li>Qixiang Ye Qixiang Ye is a full professor with the University of Chinese Academy of Sciences since 2016. He is one of the funding directors of pattern recognition and intelligent system development (Pri-SDL) lab. He received the B.S. and M.S. degrees in mechanical and electrical engineering from Harbin Institute of Technology, China, respectively, and the Ph.D. degree from the Institute of Computing Technology, Chinese Academy of Sciences. He was a visiting assistant professor with the Institute of Advanced Computer Studies (UMIACS), University of Maryland, College Park until 2013 and a visiting scholar of Duke University EECS in 2016. His research interests include visual object detection and machine learning, especially for feature representation learning, weakly supervised learning, self-supervised learning for visual object sensing. With more than 80 papers published in refereed conferences and journals including IEEE T-ITS, TIP, TNN, T-PAMI, CVPR, ICCV, ECCV, AAAI, and NeurIPS. Dr. Ye received the Sony Outstanding Paper Award and the LuJiaXi Young Researcher Award. He is an SPC of IJCAI 2020 and 2021 and on the editor boards of IEEE Transactions on Intelligent Transportation System and IEEE Transactions on Circuit and System on Video Technology.</li>
</ul>
<p><img src="./Xiaopeng_Hong.jpg" alt="Xiaopeng Hong"></p>
<ul>
<li>Xiaopeng Hong is a distinguished research fellow at Xi&rsquo;an Jiaotong University, PRC. He had been a senior researcher/adjunct professor with University of Oulu, Finland until 2019.  He has authored over 50 articles in top-tier journals and conferences such as IEEE T-PAMI and CVPR. He has served as an area chair/SPC for ACM MM 21/20, AAAI 21 and IJCAI21, a guest editor for Pattern Recognition Letter and Signal, Image and Video Processing, and a co-organizer of six interaction workshops and challenges in conjunction with CVPR, ACM MM, FG, and ACCV. His current research interests include visual surveillance, continual learning, robotic planning, and micro-expression analysis. His studies about subtle facial movement analysis have been reported by International media like MIT Technology Review and been awarded the IEEE Finland Section best student conference paper of 2020.</li>
</ul>
<p><img src="./Xiaoshan_Yang.jpg" alt="Xiaoshan Yang"></p>
<ul>
<li>Xiaoshan Yang received the Ph.D. degree in pattern recognition and intelligent systems from Institute of Automation, Chinese Academy of Sciences in 2016. He is currently an Associate Professor with the Institute of Automation, Chinese Academy of Sciences. His research focuses on data-driven and knowledge-guided multimedia content understanding. He has authored or co-authored more than 40 journal/conference papers, where 22 of them are IEEE/ACM transactions or CCF-A conferences, e.g., IEEE TMM, IEEE TIP, IEEE TCYB, ACM TOMM, IEEE CVPR, ACM MM and AAAI. He won the President Award of Chinese Academy of Sciences in 2016, the Excellent Doctoral Dissertation of Chinese Academy of Sciences in 2017, the CCF-Tencent Rhino Bird Excellence Award in 2018. He was an area chair of ICPR 2020, TPC member of ACM multimedia 2018/2019/2020/2021, reviewer of IEEE TMM/IEEE TCSVT/PR/ACM TIST/ACM TOMM/IEEE CVPR/IEEE ICCV/ACM MM.</li>
</ul>
<p><img src="./Jie_Chen.jpg" alt="Jie Chen"></p>
<ul>
<li>Jie Chen received the MSc and PhD degrees from the Harbin Institute of Technology, China, in 2002 and 2007, respectively. He joined the faculty with the Graduate School in Shenzhen, Peking University, in 2019, where he is currently an associate professor with the School of Electronic and Computer Engineering, Peking University. Since 2018, he has been working with the Peng Cheng Laboratory, China. From 2007 to 2018, he worked as a senior researcher with the Center for Machine Vision and Signal Analysis, University of Oulu, Finland. In 2012 and 2015, he visited the Computer Vision Laboratory, University of Maryland and School of Electrical and Computer Engineering, Duke University respectively. He was a cochair of International Workshops at ACCV, CVPR, ICCV and ECCV. He was a guest editor of special issues for IEEE TPAMI, IJCV and Neurocomputing. His research interests include deep learning, computer vision, and medical image analysis. He is an Associate Editor of the Visual Computer. He is a member of the IEEE.</li>
</ul>
<blockquote>
<h3 id="preference-for-half-day-event-and-rough-program-outline">Preference for half-day event and Rough program outline</h3>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">Time</th>
<th style="text-align:center">Event</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">8:50~9:00</td>
<td style="text-align:center">Opening</td>
</tr>
<tr>
<td style="text-align:center">9:00~ 9:30</td>
<td style="text-align:center">Invited Talk by Li Zhang (Talk 1)</td>
</tr>
<tr>
<td style="text-align:center">9:30~ 10:00</td>
<td style="text-align:center">Invited Talk by S. Kevin Zhou (Talk 2)</td>
</tr>
<tr>
<td style="text-align:center">10:00~10:30</td>
<td style="text-align:center">Invited Talk by Qixiang Ye (Talk 3)</td>
</tr>
<tr>
<td style="text-align:center">10:30~11:00</td>
<td style="text-align:center">Coffee break</td>
</tr>
<tr>
<td style="text-align:center">11:00~11:20</td>
<td style="text-align:center">Invited Talk by Xiaopeng Hong (Talk 4)</td>
</tr>
<tr>
<td style="text-align:center">11:20~11:40</td>
<td style="text-align:center">Invited Talk by Xiaoshan Yang (Talk 5)</td>
</tr>
<tr>
<td style="text-align:center">11:40~12:00</td>
<td style="text-align:center">Invited Talk by Jie Chen (Talk 6)</td>
</tr>
<tr>
<td style="text-align:center">12:00~12:10</td>
<td style="text-align:center">Closing Remarks</td>
</tr>
</tbody>
</table>
<blockquote>
<h3 id="motivation">Motivation</h3>
<p>Recent deep learning methods rely on a large amount of labeled data to achieve high performance. These methods may be impractical in some scenarios, where manual data annotation is costly or the samples of certain categories are scarce (e.g., tumor lesions, endangered animals and rare individual activities). When only limited annotated samples are available, these methods usually suffer the overfitting problem severely, the performance usually degrades significantly. In contrast, humans can recognize the objects in the images rapidly and correctly with their prior knowledge after exposed to only a few annotated samples. To simulate the learning schema of humans and relieve the reliance on the large-scale annotation benchmarks, researchers start shifting towards the few-shot learning problem: they try to learn a model to correctly recognize novel categories with only a few annotated samples.</p>
</blockquote>
<blockquote>
<h3 id="course-description">Course description</h3>
<p>Substantial progress has been made in computer vision tasks. This attributes to large-scale datasets with precise annotations and convolutional neural networks (CNNs) capable of absorbing the annotation information. However, annotating large-scale datasets with semantic masks is expensive, laborious, or even impractical, which violates the principle of cognitive learning where models should be constructed based on few supervisions. In practical applications, such as medical processing, it is usually difficult to collect annotated images since medical experts are usually quite busy but annotating medical images is time consuming. In this tutorial, we propose to introduce few-shot learning methods, which target at finding features that can generalize well to novel classes. With this tutorial, we give an overview about the recent progress in few-shot vision recognition, few-shot object detection, few-shot segmentation, few-shot medical image processing, and few-shot multi-modality tasks. It is expected that such a tutorial could inspire more cutting-edge methods in both the computer vision and multi-media areas in the near future.</p>
</blockquote>
<blockquote>
<h3 id="topics">Topics</h3>
<blockquote>
<h4 id="topic-1-few-shot-learning-fundamental-li-zhang">Topic 1. Few-shot Learning: Fundamental (Li Zhang)</h4>
<p>Few-shot learning (FSL) aims to recognize target classes by adapting the prior knowledge learned from source classes. Such knowledge usually resides in a deep embedding model for a general matching purpose of the support and query image pairs. In this talk, I will first introduce a conceptually simple, flexible, and general framework for few-shot learning, called the Relation Network (RN). It learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Then we repurpose the contrastive learning with Noise Contrastive Estimation (NCE) for such matching to learn a few-shot embedding model in a supervised manner, exploiting the patch-wise relationship to substantially improve the popular infoNCE.</p>
</blockquote>
<blockquote>
<h4 id="topic-2-few-shot-learning-for-medical-image-processing-s-kevin-zhou">Topic 2. Few-shot Learning for Medical Image Processing (S. Kevin Zhou)</h4>
<p>Artificial intelligence or deep learning technologies have gained prevalence in solving medical imaging tasks. In this talk, we first review the traits that characterize medical images, such as multi-modalities, heterogeneous and isolated data, sparse and noisy labels, imbalanced samples. We then summarize some properties of deep learning technologies, including the paradigm shift from &ldquo;small task, big data&rdquo; to &ldquo;big task, small data&rdquo;. Finally, we illustrate the trends of AI technologies in medical imaging and present a multitude of algorithms:</p>
<ul>
<li><strong>Annotation-efficient methods</strong> that tackle medical image analysis without many labelled instances, including one-shot or label-free inference approaches.</li>
<li><strong>Universal models</strong> that learn “common + specific” feature representations for multi-domain tasks to unleash the potential of ‘bigger data’, which are formed by integrating multiple datasets associated with tasks of interest into one use.</li>
<li><strong>&ldquo;Deep learning + knowledge modeling&rdquo; approaches</strong>, which combine machine learning with domain knowledge to enable start-of-the-art performances for many tasks of medical image reconstruction, recognition, segmentation, and parsing.</li>
</ul>
</blockquote>
<blockquote>
<h4 id="topic-3-few-shot-semantic-segmentation-qixiang-ye">Topic 3. Few-shot semantic segmentation (Qixiang ye)</h4>
<p>Few-shot semantic segmentation remains an open problem for the lack of effective method to handle the semantic misalignment between objects. In this talk, we introduce a part-based semantic transform (PST), and target at aligning object semantics in support images with those in query images by semantic decomposition-and-match. The semantic decomposition process is implemented with prototype mixture models (PMMs), which use an Expectation Maximization (EM) algorithm to decompose object semantics into multiple prototypes corresponding to object parts. The semantic match between prototypes is performed with a min-cost flow module, which encourages correct correspondence while depressing mismatches between object parts. With semantic decomposition-and-match, PST enforces network&rsquo;s tolerance to objects' appearance and/or pose variation, and facilities channel-wised and spatial semantic activation of objects in query images. In the PST framework, we furhter propose anti-aliasing semantic reconstruction (ASR), provides a systematic yet interpretable solution for few-shot learning problems. Extensive experiments on PASCAL VOC and MS COCO datasets show that ASR achieves strong results compared with the prior works. (ECCV2020, CVPR2021, TNNLS2021, TIP-2020)</p>
</blockquote>
<blockquote>
<h4 id="topic-4-few-shot-incremental-learning-xiaopeng-hong">Topic 4. Few-shot Incremental Learning (Xiaopeng Hong)</h4>
<p>Recently, graph-based methods have achieved great success in the few-shot classification task by capturing the instance-level relations of samples. However, most of them ignore exploring the class-level knowledge that can be easily learned by humans from just a handful of samples. This talk introduces an Explicit Class Knowledge Propagation Network (ECKPN), which can explicitly learn the richer class knowledge to guide the graph-based inference of query samples. Meanwhile, this talk also discusses the challenges of applying few-shot learning in multimedia. (1) There is always a large modality gap between different modalities, which makes it difficult to learn effective multimodal features with only a few labeled samples. (2) Although there are already many successful few-shot learning methods in image classification, the multimodal few-shot learning is much more challenging due to the complexity of the multimodal data. To deal with these challenges, a two-stream graph network-based multimodal few-shot learning approach is presented in this talk.</p>
</blockquote>
<blockquote>
<h4 id="topic-5-few-shot-learning-for-multi-modality-tasks-xiaoshan-yang">Topic 5. Few-shot Learning for Multi-Modality Tasks (Xiaoshan Yang)</h4>
<p>The ability to incrementally learn new classes is crucial to the development of real-world artificial intelligence systems. We focus on a challenging but practical few-shot class-incremental learning (FSCIL) problem. FSCIL requires CNN models to incrementally learn new classes from very few labelled samples, without forgetting the previously learned ones. To address this problem, we represent the knowledge using a neural gas (NG) network, which can learn and preserve the topology of the feature manifold formed by different classes. On this basis, we propose the TOpology-Preserving knowledge InCrementer (TOPIC) framework. TOPIC mitigates the forgetting of the old classes by stabilizing NG&rsquo;s topology and improves the representation learning for few-shot new classes by growing and adapting NG to new training samples. We further introduce the graph-based distillation approach into FSCIL and propose the exemplar relation distillation incremental learning framework to balance the tasks of old-knowledge preserving and new-knowledge adaptation. A large number of experiments show that our method substantially outperforms other CIL and FSCIL methods on CIFAR100, miniImageNet, and CUB200 datasets.</p>
</blockquote>
<blockquote>
<h4 id="topic-6-few-shot-medical-image-segmentation-jie-chen">Topic 6. Few-shot Medical Image Segmentation (Jie Chen)</h4>
<p>Few-shot learning is a typical scenario for medical image analysis. In medical domain, manual data annotation is very expensive and the samples of certain categories are scarce (e.g., tumor lesions). In this talk, we will focus on medical image segmentation, i.e., nuclear instance and polyp segmentation. For the nuclear instance segmentation, we propose a novel direction guided network (DGN). Specifically, we first define the direction feature to represent the spatial relationship between pixels. Based on direction feature, we propose a direction difference map (DDM) to reflect the direction difference of adjacent pixels and guide network to identify the instance boundary, especially the boundary of overlapping regions. Further, we propose a direction-guided refinement module, which effectively integrates auxiliary tasks and is used as a plug-and-play module to form the refining network for nuclear instance segmentation. For the polyp segmentation, we propose a Learnable Oriented-Derivative Network (LOD-Net) to refine the accuracy of boundary predictions for polyps. In particular, it firstly calculates eight oriented derivatives at each pixel for a polyp. It then selects those pixels with large oriented-derivative values to constitute a candidate border region of a polyp. It finally refines boundary prediction by fusing border region features and also those high-level semantic features calculated by a backbone network.</p>
</blockquote>
</blockquote>
<blockquote>
<h3 id="audience">Audience</h3>
<p>According to our experience in the previous workshop, it is expected to get about 100 attendees. Anticipated target audience would be intermediate and advanced researchers.</p>
</blockquote>
<blockquote>
<h3 id="description-of-materials">Description of materials</h3>
<p>Slides will be distributed to the attendees.</p>
</blockquote>
<blockquote>
<h3 id="key-publications-of-proposers-on-the-tutorial-topic">Key Publications of Proposers On the Tutorial Topic</h3>
<ul>
<li>Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, Timothy M. Hospedales, Learning to Compare: Relation Network for Few-Shot Learning, CVPR 2018 (1412 citations by Google Scholar)</li>
<li>Yikai Wang, Li Zhang, Yuan Yao, Yanwei Fu, How to trust unlabeled data? Instance Credibility Inference for Few-Shot Learning, IEEE TPAMI 2021</li>
<li>Chen Liu, Yanwei Fu, Chengming Xu, Siqian Yang, Jilin Li, Chengjie Wang, Li Zhang, Learning a Few-shot Embedding Model with Contrastive Learning, AAAI, 2021</li>
<li>Yikai Wang, Chengming Xu, Chen Liu, Li Zhang, Yanwei Fu, Instance Credibility Inference for Few-Shot Learning, CVPR, 2020</li>
<li>Hongguang Zhang, Li Zhang, Xiaojuan Qi, Hongdong Li, Philip H.S. Torr, Piotr Koniusz, Few-shot Action Recognition with Permutation-invariant Attention, ECCV, 2020</li>
<li>Yuqian Fu, Li Zhang, Junke Wang, Yanwei Fu, Yu-Gang Jiang, Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition, ACM MM 2020</li>
<li>S. Kevin Zhou, H. Greenspan, C. Davatzikos, J.S. Duncan, B. van Ginneken, A. Madabhushi, J.L. Prince, D. Rueckert, and R.M. Summers, “A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises,” Proceedings of the IEEE, 2021</li>
<li>Q. Yao, L. Xiao, P. Liu, and S. Kevin Zhou, “Label-free segmentation of COVID-19 lesions in lung CT,” IEEE Trans. on Medical Imaging, 2021.</li>
<li>G. Shi, L. Xiao, Y. Chen, and S. Kevin Zhou, “Marginal loss and exclusion loss for partially supervised multi-organ segmentation,” Medical Image Analysis, 2021.</li>
<li>H. Li, H. Han, Z. Li, L. Wang, Z. Wu, J. Lu, and S. Kevin Zhou, “High-resolution chest X-ray bone suppression using unpaired CT structural priors,” IEEE Trans. on Medical Imaging, 2020.</li>
<li>C. Huang, H. Han, Q. Yao, S. Zhu, and S. Kevin Zhou, “3D U2Net: A 3D universal u-net for multi-domain medical image segmentation,” MICCAI, 2019.</li>
<li>B. Li, B. Yang, C. Liu, F. Liu, R. Ji, Q. Ye, &ldquo;Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection,&rdquo; CVPR 2021.</li>
<li>B. Liu, Y. Ding, J. Jiao, X. Ji, Q. Ye, Anti-aliasing Semantic Reconstruction for Few-Shot Semantic Segmentation, CVPR 2021.</li>
<li>B. Yang, C Liu, B. Li, J. Jiao,  Q. Ye, &ldquo;Prototype Mixture Models for Few-shot Semantic Segmentation,&rdquo; ECCV 2020.</li>
<li>B. Liu, J. Jiao, Q. Ye, &ldquo;Harmonic Feature Activation for Few-Shot Semantic Segmentation,&rdquo; IEEE Trans. Image Process, 30(2):3142 - 3153, 2021</li>
<li>Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, Yihong Gong. Few-Shot Class-Incremental Learning.  CVPR, 2020. Oral</li>
<li>Songlin Dong, Xiaopeng Hong, Xiaoyu Tao, Xinyuan Chang, Xing Wei, Yinghong Gong. Few-Shot Class-Incremental Learning via Relation Knowledge Distillation. AAAI, 2021.</li>
<li>Xiaoyu Tao, Xinyuan Chang, Xiaopeng Hong, Songlin Dong, Xing Wei, Yihong Gong. Topology-Preserving Class-Incremental Learning. ECCV, 2020.</li>
<li>Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Yihong Gong. Bi-objective Continual Learning: Learning ‘New’ while Consolidating ‘Known,’ AAAI, 2020.</li>
<li>Chaofan Chen, Xiaoshan Yang, Changsheng Xu, Xuhui Huang, Zhe Ma: ECKPN: Explicit Class Knowledge Propagation Network for Transductive Few-shot Learning, CVPR 2021</li>
<li>Yi Huang, Xiaoshan Yang, Junyu Gao, Jitao Sang, Changsheng Xu, Knowledge-driven Egocentric Multimodal Activity Recognition, TOMM, 2020</li>
<li>Fan Qi, Xiaoshan Yang, Changsheng Xu, Emotion Knowledge Driven Video Highlight Detection, IEEE TMM, 2020</li>
<li>Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, Matti Pietikäinen; Deep Learning for Generic Object Detection: A Survey, IJCV, 2019, (780 citations by Google Scholar)</li>
<li>Xiawu Zheng, Rongrong Ji_, Qiang Wang, Yuhang Chen, Baochang Zhang, Jie Chen, Qixiang Ye, Feiyue Huang, Yonghong Tian, MIGO-NAS: Towards Fast and Generalizable Neural Architecture Search, IEEE TPAMI, 2021</li>
<li>Hongliang He, Yao Ding, Guoli Song, Pengxu Wei, Jie Chen, DGN: Direction Guided Network for Nuclear Instance Segmentation, ICCV</li>
<li>Mengjun Cheng, Zishang Kong, Jie Chen, Learnable Oriented-Derivative Network for Polyp Segmentation, MICCAI 2021</li>
</ul>
</blockquote>
<blockquote>
<h3 id="main-contact">Main contact:</h3>
<p>Jie Chen<br>
Peking University, China<br>
<a href="mailto:jiechen2019@pku.edu.cn">jiechen2019@pku.edu.cn</a></p>
</blockquote>


        </main>
      </div>
      <div id="navigation">
        <a class="nav nav-next" href="./post/" title="Posts"><i class="fa fa-chevron-right"></i></a>
      </div>
    </div>
    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="./js/clipboard.min.js?1637301373"></script>
    <script src="./js/perfect-scrollbar.min.js?1637301373"></script>
    <script src="./js/perfect-scrollbar.jquery.min.js?1637301373"></script>
    <script src="./js/jquery.svg.pan.zoom.js?1637301373"></script>
    <script src="./js/featherlight.min.js?1637301373"></script>
    <script src="./js/modernizr.custom-3.6.0.js?1637301373"></script>
    <script src="./js/mermaid.min.js?1637301373"></script>
    <script>
      if (typeof mermaid != 'undefined' && typeof mermaid.mermaidAPI != 'undefined') {
        mermaid.mermaidAPI.initialize( Object.assign( { "securityLevel": "antiscript" }, JSON.parse("{ \"startOnLoad\": true }"), { startOnLoad: false } ) );
      }
    </script>
    <script src="./js/relearn.js?1637301373"></script>
  </body>
</html>
